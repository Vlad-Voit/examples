{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqWb4lqagrtv",
        "outputId": "ad439fbd-23f5-492b-dcf5-a4574a60efec"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiDoMoVPjtFU"
      },
      "source": [
        "**Bag-of-words (CountVectorizer)**\n",
        "\n",
        "Рассмотрим  самый простой способ приведения текста к набору чисел. Для каждого слова посчитаем, как часто оно встречается в тексте. Результаты запишем в таблицу. Строки будут представлять тексты, столбцы -- слова. Если на пересечении строки с столбца стоит число 5, значит данное слово встретилось в данном тексте 5 раз. В большинстве ячеек будут нули. Поэтому хранить это всё удобнее в виде разреженных матриц (т.е. хранить только ненулевые значения).\n",
        "\n",
        "Таким образом, при построении \"мешка слов\" можно выделить следующие действия:\n",
        "\n",
        "Токенизация.\n",
        "\n",
        "Построение словаря: собираем все слова, которые встречались в текстах и пронумеровываем их (по алфавиту, например).\n",
        "\n",
        "Построение разреженной матрицы.\n",
        "В sklearn алгоритм приведения текста в bag-of-words реализован в виде класса CountVectorizer. Рассмотрим пример."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_n4791CiCM9",
        "outputId": "8daaaca0-475a-4821-ee3a-c17d02d9b2fd"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vectorizer = CountVectorizer()\n",
        "texts = [\"Великолепный сериал, который поможет успокоить нервы при любых стрессах и просто скрасит серые будни\",\n",
        "         \"Пожалуй, если бы я посмотрел только первые пару сезонов этого сериала, я бы с легкой руки написал ему положительную рецензию\",\n",
        "         \"В общем, если создатели этого сериала не вернут всё на круги своя, то рейтинги следующих сезонов будут становится все ниже и ниже, а зрительская аудитория будет все меньше и меньше.\"]\n",
        "\n",
        "bow = count_vectorizer.fit_transform(texts)\n",
        "bow.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 48)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts"
      ],
      "metadata": {
        "id": "iXXjmTZR9ZVv",
        "outputId": "188284b5-5ae8-4ab4-8332-412d63a9cb36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Великолепный сериал, который поможет успокоить нервы при любых стрессах и просто скрасит серые будни',\n",
              " 'Пожалуй, если бы я посмотрел только первые пару сезонов этого сериала, я бы с легкой руки написал ему положительную рецензию',\n",
              " 'В общем, если создатели этого сериала не вернут всё на круги своя, то рейтинги следующих сезонов будут становится все ниже и ниже, а зрительская аудитория будет все меньше и меньше.']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjotKzMLkIjl"
      },
      "source": [
        "Результат содержит 3 строки (для 3 текстов) и 48 столбцов (для 48 разных слов). Посмотрим словарь (для иллюстрации):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i47fVP1qjfzF",
        "outputId": "480336a2-5dba-4cb8-ac67-ac2df32f11c6"
      },
      "source": [
        "count_vectorizer.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'аудитория': 0,\n",
              " 'будет': 1,\n",
              " 'будни': 2,\n",
              " 'будут': 3,\n",
              " 'бы': 4,\n",
              " 'великолепный': 5,\n",
              " 'вернут': 6,\n",
              " 'все': 7,\n",
              " 'всё': 8,\n",
              " 'ему': 9,\n",
              " 'если': 10,\n",
              " 'зрительская': 11,\n",
              " 'который': 12,\n",
              " 'круги': 13,\n",
              " 'легкой': 14,\n",
              " 'любых': 15,\n",
              " 'меньше': 16,\n",
              " 'на': 17,\n",
              " 'написал': 18,\n",
              " 'не': 19,\n",
              " 'нервы': 20,\n",
              " 'ниже': 21,\n",
              " 'общем': 22,\n",
              " 'пару': 23,\n",
              " 'первые': 24,\n",
              " 'пожалуй': 25,\n",
              " 'положительную': 26,\n",
              " 'поможет': 27,\n",
              " 'посмотрел': 28,\n",
              " 'при': 29,\n",
              " 'просто': 30,\n",
              " 'рейтинги': 31,\n",
              " 'рецензию': 32,\n",
              " 'руки': 33,\n",
              " 'своя': 34,\n",
              " 'сезонов': 35,\n",
              " 'сериал': 36,\n",
              " 'сериала': 37,\n",
              " 'серые': 38,\n",
              " 'скрасит': 39,\n",
              " 'следующих': 40,\n",
              " 'создатели': 41,\n",
              " 'становится': 42,\n",
              " 'стрессах': 43,\n",
              " 'то': 44,\n",
              " 'только': 45,\n",
              " 'успокоить': 46,\n",
              " 'этого': 47}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIG2xjPGkVFs"
      },
      "source": [
        "Как видим, ни стемминга, ни лемматизации по умолчанию не производится.\n",
        "\n",
        "Результат преобразования (для иллюстрации):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KqQUbSjkZDj",
        "outputId": "e41e37fb-ee09-4dc6-a49b-1959d3e4d1e2"
      },
      "source": [
        "bow.todense()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
              "         0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
              "         0, 1, 0, 0, 1, 0],\n",
              "        [0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "         0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
              "         0, 0, 0, 1, 0, 1],\n",
              "        [1, 1, 0, 1, 0, 0, 1, 2, 1, 0, 1, 1, 0, 1, 0, 0, 2, 1, 0, 1, 0,\n",
              "         2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
              "         1, 0, 1, 0, 0, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xTJVmAzlWg8"
      },
      "source": [
        "**Стоп-слова**\n",
        "\n",
        "Можно легко включить отсечение стоп-слов (иллюстрация):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnfqp7K2lho0",
        "outputId": "a1631ef1-78c2-4951-9380-9b84bd374a54"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "s= stopwords.words(\"russian\")\n",
        "s"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['и',\n",
              " 'в',\n",
              " 'во',\n",
              " 'не',\n",
              " 'что',\n",
              " 'он',\n",
              " 'на',\n",
              " 'я',\n",
              " 'с',\n",
              " 'со',\n",
              " 'как',\n",
              " 'а',\n",
              " 'то',\n",
              " 'все',\n",
              " 'она',\n",
              " 'так',\n",
              " 'его',\n",
              " 'но',\n",
              " 'да',\n",
              " 'ты',\n",
              " 'к',\n",
              " 'у',\n",
              " 'же',\n",
              " 'вы',\n",
              " 'за',\n",
              " 'бы',\n",
              " 'по',\n",
              " 'только',\n",
              " 'ее',\n",
              " 'мне',\n",
              " 'было',\n",
              " 'вот',\n",
              " 'от',\n",
              " 'меня',\n",
              " 'еще',\n",
              " 'нет',\n",
              " 'о',\n",
              " 'из',\n",
              " 'ему',\n",
              " 'теперь',\n",
              " 'когда',\n",
              " 'даже',\n",
              " 'ну',\n",
              " 'вдруг',\n",
              " 'ли',\n",
              " 'если',\n",
              " 'уже',\n",
              " 'или',\n",
              " 'ни',\n",
              " 'быть',\n",
              " 'был',\n",
              " 'него',\n",
              " 'до',\n",
              " 'вас',\n",
              " 'нибудь',\n",
              " 'опять',\n",
              " 'уж',\n",
              " 'вам',\n",
              " 'ведь',\n",
              " 'там',\n",
              " 'потом',\n",
              " 'себя',\n",
              " 'ничего',\n",
              " 'ей',\n",
              " 'может',\n",
              " 'они',\n",
              " 'тут',\n",
              " 'где',\n",
              " 'есть',\n",
              " 'надо',\n",
              " 'ней',\n",
              " 'для',\n",
              " 'мы',\n",
              " 'тебя',\n",
              " 'их',\n",
              " 'чем',\n",
              " 'была',\n",
              " 'сам',\n",
              " 'чтоб',\n",
              " 'без',\n",
              " 'будто',\n",
              " 'чего',\n",
              " 'раз',\n",
              " 'тоже',\n",
              " 'себе',\n",
              " 'под',\n",
              " 'будет',\n",
              " 'ж',\n",
              " 'тогда',\n",
              " 'кто',\n",
              " 'этот',\n",
              " 'того',\n",
              " 'потому',\n",
              " 'этого',\n",
              " 'какой',\n",
              " 'совсем',\n",
              " 'ним',\n",
              " 'здесь',\n",
              " 'этом',\n",
              " 'один',\n",
              " 'почти',\n",
              " 'мой',\n",
              " 'тем',\n",
              " 'чтобы',\n",
              " 'нее',\n",
              " 'сейчас',\n",
              " 'были',\n",
              " 'куда',\n",
              " 'зачем',\n",
              " 'всех',\n",
              " 'никогда',\n",
              " 'можно',\n",
              " 'при',\n",
              " 'наконец',\n",
              " 'два',\n",
              " 'об',\n",
              " 'другой',\n",
              " 'хоть',\n",
              " 'после',\n",
              " 'над',\n",
              " 'больше',\n",
              " 'тот',\n",
              " 'через',\n",
              " 'эти',\n",
              " 'нас',\n",
              " 'про',\n",
              " 'всего',\n",
              " 'них',\n",
              " 'какая',\n",
              " 'много',\n",
              " 'разве',\n",
              " 'три',\n",
              " 'эту',\n",
              " 'моя',\n",
              " 'впрочем',\n",
              " 'хорошо',\n",
              " 'свою',\n",
              " 'этой',\n",
              " 'перед',\n",
              " 'иногда',\n",
              " 'лучше',\n",
              " 'чуть',\n",
              " 'том',\n",
              " 'нельзя',\n",
              " 'такой',\n",
              " 'им',\n",
              " 'более',\n",
              " 'всегда',\n",
              " 'конечно',\n",
              " 'всю',\n",
              " 'между']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLQoHd4TktxL",
        "outputId": "98950820-7f2f-478f-9743-3092a1c7587a"
      },
      "source": [
        "count_vectorizer = CountVectorizer(stop_words=s)\n",
        "bow = count_vectorizer.fit_transform(texts)\n",
        "count_vectorizer.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'аудитория': 0,\n",
              " 'будни': 1,\n",
              " 'будут': 2,\n",
              " 'великолепный': 3,\n",
              " 'вернут': 4,\n",
              " 'всё': 5,\n",
              " 'зрительская': 6,\n",
              " 'который': 7,\n",
              " 'круги': 8,\n",
              " 'легкой': 9,\n",
              " 'любых': 10,\n",
              " 'меньше': 11,\n",
              " 'написал': 12,\n",
              " 'нервы': 13,\n",
              " 'ниже': 14,\n",
              " 'общем': 15,\n",
              " 'пару': 16,\n",
              " 'первые': 17,\n",
              " 'пожалуй': 18,\n",
              " 'положительную': 19,\n",
              " 'поможет': 20,\n",
              " 'посмотрел': 21,\n",
              " 'просто': 22,\n",
              " 'рейтинги': 23,\n",
              " 'рецензию': 24,\n",
              " 'руки': 25,\n",
              " 'своя': 26,\n",
              " 'сезонов': 27,\n",
              " 'сериал': 28,\n",
              " 'сериала': 29,\n",
              " 'серые': 30,\n",
              " 'скрасит': 31,\n",
              " 'следующих': 32,\n",
              " 'создатели': 33,\n",
              " 'становится': 34,\n",
              " 'стрессах': 35,\n",
              " 'успокоить': 36}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SL9AinMEmtDl",
        "outputId": "1803d028-0697-485d-f5fd-36674bf14b0c"
      },
      "source": [
        "bow.todense()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
              "         0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,\n",
              "         1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0,\n",
              "         0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLCOUVlopRSe"
      },
      "source": [
        "**Параметр min_df**\n",
        "\n",
        "Помимо стоп-слов есть и другие способы отсечения лишнего. Например, можно откидывать слова, которые встречаются слишком редко, с помощью параметра min_df. \n",
        "Установив min_df=2 мы откинем, все слова, которые встречаются менее, чем в 2 документах."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4duZwfQnl6F",
        "outputId": "e97ff21f-a3fd-40fb-e2bd-6a38b0fadbd4"
      },
      "source": [
        "count_vectorizer = CountVectorizer(min_df=2)\n",
        "bow = count_vectorizer.fit_transform(texts)\n",
        "count_vectorizer.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'если': 0, 'сезонов': 1, 'сериала': 2, 'этого': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WktzloToplVe"
      },
      "source": [
        "**Биграммы, триграммы, n-граммы**\n",
        "\n",
        "По умолчанию bag-of-words (как следует из названия) представляет собой просто мешок слов. То есть для него предложения \"It's not good, it's bad!\" и \"It's not bad, it's good!\" абсолютно эквивалентны. Понятно, что при этом теряется много информации. Можно рассматривать не только отдельные слова, а последовательности длиной из 2 слов (биграммы), из 3 слов (триграммы) или в общем случае из n слов (n-граммы). На практике обычно задаётся диапазон от 1 до n.\n",
        "\n",
        "Рассмотрим пример:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87Yk3Sm7ppDs",
        "outputId": "efc3c18d-7995-47fa-d96d-007c2c197e9f"
      },
      "source": [
        "count_vectorizer = CountVectorizer(ngram_range=(1,2),  min_df=2)\n",
        "bow = count_vectorizer.fit_transform(texts)\n",
        "count_vectorizer.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'если': 0, 'сезонов': 1, 'сериала': 2, 'этого': 3, 'этого сериала': 4}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQsOLS7Jqvyd"
      },
      "source": [
        "**Ограничение количества признаков**\n",
        "\n",
        "Понятно, что с ростом n количество выделенных n-грамм быстро растёт. Для ограничения количества признаков можно использовать параметр max_features. В этом случае будет создано не более max_features признаков (будут выбраны самые часто встречающиеся слова и последовательности слов). Например:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cNGkR9Cqyrm",
        "outputId": "0d1a90d7-fb9d-40f9-891c-9025b7ea32d7"
      },
      "source": [
        "count_vectorizer = CountVectorizer(ngram_range=(1,2),  max_features=25)\n",
        "bow = count_vectorizer.fit_transform(texts)\n",
        "count_vectorizer.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'бы': 0,\n",
              " 'все': 1,\n",
              " 'если': 2,\n",
              " 'меньше': 3,\n",
              " 'ниже': 4,\n",
              " 'положительную': 5,\n",
              " 'положительную рецензию': 6,\n",
              " 'поможет': 7,\n",
              " 'поможет успокоить': 8,\n",
              " 'посмотрел': 9,\n",
              " 'посмотрел только': 10,\n",
              " 'при': 11,\n",
              " 'при любых': 12,\n",
              " 'просто': 13,\n",
              " 'просто скрасит': 14,\n",
              " 'рейтинги следующих': 15,\n",
              " 'рецензию': 16,\n",
              " 'руки': 17,\n",
              " 'руки написал': 18,\n",
              " 'своя': 19,\n",
              " 'своя то': 20,\n",
              " 'сезонов': 21,\n",
              " 'сериала': 22,\n",
              " 'этого': 23,\n",
              " 'этого сериала': 24}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HCxJRrpsDR-"
      },
      "source": [
        "**TF-IDF**\n",
        "\n",
        "У подхода bag-of-words есть существенный недостаток. Если слово встречается 5 раз в конкретном документе, но и в других документах тоже встречается часто, то его наличие в документе не особо-то о чём-то говорит. Если же слово 5 раз встречается в конкретном документе, но в других документах встречается редко, то его наличие (да ещё и многократное) позволяет хорошо отличать этот документ от других. Однако с точки зрения bag-of-words различий не будет: в обеих ячейках будет просто число 5.\n",
        "\n",
        "Отчасти это решается исключением стоп-слов (и слишком часто встречающихся слов), но лишь отчасти. Другой идеей является отмасштабировать получившуюся таблицу с учётом \"редкости\" слова в наборе документов (т.е. с учётом информативности слова).\n",
        "\n",
        "tfidf=tf∗idf\n",
        "\n",
        "idf=log((N+1)/(Nw+1))+1\n",
        "\n",
        "Здесь tf это частота слова в тексте (то же самое, что в bag of words), N - общее число документов, Nw - число документов, содержащих данное слово.\n",
        "\n",
        "То есть для каждого слова считается отношение общего количества документов к количеству документов, содержащих данное слово (для частых слов оно будет ближе к 1, для редких слов оно будет стремиться к числу, равному количеству документов), и на логарифм от этого числа умножается исходное значение bag-of-words (к числителю и знаменателю прибавляется единичка, чтобы не делить на 0, и к логарифму тоже прибавляется единичка, но это уже технические детали). После этого в sklearn ещё проводится L2-нормализация каждой строки.\n",
        "\n",
        "В sklearn есть  класс для поддержки TF-IDF: TfidfVectorizer, рассмотрим его."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dkZYiTnsrdw",
        "outputId": "4aba1b49-9b82-46fc-ab63-f44fb2fac539"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf = tfidf_vectorizer.fit_transform(texts)\n",
        "tfidf_vectorizer.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'аудитория': 0,\n",
              " 'будет': 1,\n",
              " 'будни': 2,\n",
              " 'будут': 3,\n",
              " 'бы': 4,\n",
              " 'великолепный': 5,\n",
              " 'вернут': 6,\n",
              " 'все': 7,\n",
              " 'всё': 8,\n",
              " 'ему': 9,\n",
              " 'если': 10,\n",
              " 'зрительская': 11,\n",
              " 'который': 12,\n",
              " 'круги': 13,\n",
              " 'легкой': 14,\n",
              " 'любых': 15,\n",
              " 'меньше': 16,\n",
              " 'на': 17,\n",
              " 'написал': 18,\n",
              " 'не': 19,\n",
              " 'нервы': 20,\n",
              " 'ниже': 21,\n",
              " 'общем': 22,\n",
              " 'пару': 23,\n",
              " 'первые': 24,\n",
              " 'пожалуй': 25,\n",
              " 'положительную': 26,\n",
              " 'поможет': 27,\n",
              " 'посмотрел': 28,\n",
              " 'при': 29,\n",
              " 'просто': 30,\n",
              " 'рейтинги': 31,\n",
              " 'рецензию': 32,\n",
              " 'руки': 33,\n",
              " 'своя': 34,\n",
              " 'сезонов': 35,\n",
              " 'сериал': 36,\n",
              " 'сериала': 37,\n",
              " 'серые': 38,\n",
              " 'скрасит': 39,\n",
              " 'следующих': 40,\n",
              " 'создатели': 41,\n",
              " 'становится': 42,\n",
              " 'стрессах': 43,\n",
              " 'то': 44,\n",
              " 'только': 45,\n",
              " 'успокоить': 46,\n",
              " 'этого': 47}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GizGEHSZs76X"
      },
      "source": [
        "Словарь содержит те же 48 значений, которые были бы и для CountVectorizer. Но значения в таблице другие:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gs_9jsJjtD2H",
        "outputId": "1977bccd-4aaf-4e18-c50d-9f35244c944f"
      },
      "source": [
        "tfidf.todense()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[0.        , 0.        , 0.2773501 , 0.        , 0.        ,\n",
              "         0.2773501 , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.2773501 , 0.        , 0.        ,\n",
              "         0.2773501 , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.2773501 , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.2773501 , 0.        , 0.2773501 ,\n",
              "         0.2773501 , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.2773501 , 0.        , 0.2773501 , 0.2773501 ,\n",
              "         0.        , 0.        , 0.        , 0.2773501 , 0.        ,\n",
              "         0.        , 0.2773501 , 0.        ],\n",
              "        [0.        , 0.        , 0.        , 0.        , 0.48065817,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.24032909,\n",
              "         0.18277647, 0.        , 0.        , 0.        , 0.24032909,\n",
              "         0.        , 0.        , 0.        , 0.24032909, 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.24032909, 0.24032909,\n",
              "         0.24032909, 0.24032909, 0.        , 0.24032909, 0.        ,\n",
              "         0.        , 0.        , 0.24032909, 0.24032909, 0.        ,\n",
              "         0.18277647, 0.        , 0.18277647, 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.24032909, 0.        , 0.18277647],\n",
              "        [0.18162735, 0.18162735, 0.        , 0.18162735, 0.        ,\n",
              "         0.        , 0.18162735, 0.36325471, 0.18162735, 0.        ,\n",
              "         0.13813228, 0.18162735, 0.        , 0.18162735, 0.        ,\n",
              "         0.        , 0.36325471, 0.18162735, 0.        , 0.18162735,\n",
              "         0.        , 0.36325471, 0.18162735, 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.18162735, 0.        , 0.        , 0.18162735,\n",
              "         0.13813228, 0.        , 0.13813228, 0.        , 0.        ,\n",
              "         0.18162735, 0.18162735, 0.18162735, 0.        , 0.18162735,\n",
              "         0.        , 0.        , 0.13813228]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrDeGwMotYl3"
      },
      "source": [
        "Ненулевые значения находятся на тех же местах, но отмасштабированы в зависимости от частоты слов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHuvnTYWt4hX"
      },
      "source": [
        "**Параметр sublinear_tf**\n",
        "\n",
        "Большая часть параметров у CountVectorizer и TfidfVectorizer одинакова. Но у TfidfVectorizer есть один важный дополнительный параметр.\n",
        "\n",
        "Как видно из формулы tfidf = tf * idf, если слово будет встречаться не один, а два раза, то tfidf вырастет в два раза. Если слово будет встречаться не один, а 10 раз, то tfidf вырастет почти в 10 раз.  В качестве примера добавим в третью строку ещё пару слов меньше"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IL90-pk_uNqp",
        "outputId": "291477a2-7cc8-4a3f-8c7f-d4f66bcaa3fb"
      },
      "source": [
        "texts = [\"Великолепный сериал, который поможет успокоить нервы при любых стрессах и просто скрасит серые будни\",\n",
        "         \"Пожалуй, если бы я посмотрел только первые пару сезонов этого сериала, я бы с легкой руки написал ему положительную рецензию\",\n",
        "         \"В общем, если создатели этого сериала не вернут всё на круги своя, то рейтинги следующих сезонов будут становится все ниже и ниже, а зрительская аудитория будет все меньше и меньше и меньше и меньше.\"]\n",
        "TfidfVectorizer().fit_transform(texts).todense()[2]         "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[0.15373049, 0.15373049, 0.        , 0.15373049, 0.        ,\n",
              "         0.        , 0.15373049, 0.30746099, 0.15373049, 0.        ,\n",
              "         0.116916  , 0.15373049, 0.        , 0.15373049, 0.        ,\n",
              "         0.        , 0.61492198, 0.15373049, 0.        , 0.15373049,\n",
              "         0.        , 0.30746099, 0.15373049, 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.15373049, 0.        , 0.        , 0.15373049,\n",
              "         0.116916  , 0.        , 0.116916  , 0.        , 0.        ,\n",
              "         0.15373049, 0.15373049, 0.15373049, 0.        , 0.15373049,\n",
              "         0.        , 0.        , 0.116916  ]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jzfZx7qu6wI"
      },
      "source": [
        "Значение tfidf слова \"меньше\" выросло с 0.36325471 до 0.61492198, а остальные  упали .\n",
        "\n",
        "Вопрос - хотим ли мы таких сильных изменений. Если не хотим, то можно использовать параметр sublinear_tf=True. При его использовании вместо tf будет браться 1 + log(tf). То есть по-прежнему с ростом tf будет расти и tfidf, но уже не так радикально (и соответственно остальные значения будут уменьшаться не так быстро). Для некоторых задач это может дать прирост в качестве."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_BnDi6dvTWx",
        "outputId": "246bd154-b285-42c6-c3c0-165cfdacd1a8"
      },
      "source": [
        "TfidfVectorizer(sublinear_tf=True).fit_transform(texts).todense()[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[0.18336592, 0.18336592, 0.        , 0.18336592, 0.        ,\n",
              "         0.        , 0.18336592, 0.31046549, 0.18336592, 0.        ,\n",
              "         0.13945451, 0.18336592, 0.        , 0.18336592, 0.        ,\n",
              "         0.        , 0.43756505, 0.18336592, 0.        , 0.18336592,\n",
              "         0.        , 0.31046549, 0.18336592, 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.18336592, 0.        , 0.        , 0.18336592,\n",
              "         0.13945451, 0.        , 0.13945451, 0.        , 0.        ,\n",
              "         0.18336592, 0.18336592, 0.18336592, 0.        , 0.18336592,\n",
              "         0.        , 0.        , 0.13945451]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SV85qlUN0UUK"
      },
      "source": [
        "**Стемминг**\n",
        "\n",
        "Стемминг просто отсекает то, что посчитает изменяемым окончанием слова. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MwT3S9W2wUqn",
        "outputId": "539faef1-d9ca-48b7-bb46-ea3867d490d5"
      },
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowball = SnowballStemmer(language=\"russian\")\n",
        "snowball.stem(\"сериал\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'сериа'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1c74rJQ56pm"
      },
      "source": [
        "Метод word_tokenize библиотеки nltk добавляет в словарь знаки пунктуации и слова из одной буквы!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q12kg2rcy9yh",
        "outputId": "05386547-b5c9-4b86-e07a-48edd75d961a"
      },
      "source": [
        "tokenized = nltk.word_tokenize(texts[0])\n",
        "tokenized"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Великолепный',\n",
              " 'сериал',\n",
              " ',',\n",
              " 'который',\n",
              " 'поможет',\n",
              " 'успокоить',\n",
              " 'нервы',\n",
              " 'при',\n",
              " 'любых',\n",
              " 'стрессах',\n",
              " 'и',\n",
              " 'просто',\n",
              " 'скрасит',\n",
              " 'серые',\n",
              " 'будни']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDwdKK1QzkbY",
        "outputId": "8f015ea2-34c7-4de0-8d5e-58d2e2bc9f4e"
      },
      "source": [
        "text1=[snowball.stem(w) for w in tokenized]\n",
        "text1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['великолепн',\n",
              " 'сериа',\n",
              " ',',\n",
              " 'котор',\n",
              " 'поможет',\n",
              " 'успоко',\n",
              " 'нерв',\n",
              " 'при',\n",
              " 'люб',\n",
              " 'стресс',\n",
              " 'и',\n",
              " 'прост',\n",
              " 'скрас',\n",
              " 'сер',\n",
              " 'будн']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hSVRLyP09TB"
      },
      "source": [
        "**Лемматизация**\n",
        "\n",
        "Лемматизация русских слов реализована в библиотеке pymorphy2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuBU-BnQ07_J",
        "outputId": "13cbb7b8-d94e-4c7c-efb9-3c3d7fdb2836"
      },
      "source": [
        "! pip install pymorphy2\n",
        "import pymorphy2\n",
        "morph = pymorphy2.MorphAnalyzer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████                          | 10 kB 19.0 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 20 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 30 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 40 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 51 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 55 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n",
            "Collecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Ndz3rVC821WS",
        "outputId": "b34401c4-3bd7-4c99-c3c7-ece032583806"
      },
      "source": [
        "morph.parse(\"маленького\")[0].normal_form"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'маленький'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4t1ici_t3G15",
        "outputId": "17de010c-eca3-4d40-f8a0-87552669fb1e"
      },
      "source": [
        "text2=[morph.parse(w)[0].normal_form for w in tokenized]\n",
        "text2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['великолепный',\n",
              " 'сериал',\n",
              " ',',\n",
              " 'который',\n",
              " 'помочь',\n",
              " 'успокоить',\n",
              " 'нерв',\n",
              " 'при',\n",
              " 'любой',\n",
              " 'стресс',\n",
              " 'и',\n",
              " 'просто',\n",
              " 'скрасить',\n",
              " 'серый',\n",
              " 'будни']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ShjGEfq6O0J"
      },
      "source": [
        "В библиотеке nltk есть еще  RegexpTokenizer. Он не добавляет знаки пунктуации в словарь. И в нем можно указать минимальную длину слова, добавляемого в  словарь\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_A4h9MllFzfw",
        "outputId": "3520a890-d5c0-471d-9e91-5bef01bd6e49"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokeniz = RegexpTokenizer(r'\\w{2,}')\n",
        "text3=tokeniz.tokenize(texts[0])\n",
        "text3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Великолепный',\n",
              " 'сериал',\n",
              " 'который',\n",
              " 'поможет',\n",
              " 'успокоить',\n",
              " 'нервы',\n",
              " 'при',\n",
              " 'любых',\n",
              " 'стрессах',\n",
              " 'просто',\n",
              " 'скрасит',\n",
              " 'серые',\n",
              " 'будни']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXTQS5yW5mHr",
        "outputId": "f789d319-b9a4-4551-be00-d8f9411d3448"
      },
      "source": [
        "text4=[morph.parse(w)[0].normal_form for w in text3]\n",
        "text4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['великолепный',\n",
              " 'сериал',\n",
              " 'который',\n",
              " 'помочь',\n",
              " 'успокоить',\n",
              " 'нерв',\n",
              " 'при',\n",
              " 'любой',\n",
              " 'стресс',\n",
              " 'просто',\n",
              " 'скрасить',\n",
              " 'серый',\n",
              " 'будни']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cqMe_nw0zaJ"
      },
      "source": [
        "Несмотря на то что в библиотеке scikit-learn не реализован ни один из способов нормализации (ни стемминг, ни лемматизация), CountVectorizer позволяет задать собственный токенизатор, который преобразует каждый документ в список токенов с помощью параметра tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqXUtcrK6620"
      },
      "source": [
        "def custom_tokenizer(text):\n",
        "  tokeniz = RegexpTokenizer(r'\\w{2,}')\n",
        "  text1=tokeniz.tokenize(text)\n",
        "  return [morph.parse(w)[0].normal_form for w in text1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMu0e9bkB2zk",
        "outputId": "a76fae55-9bdc-42ab-9664-eeb1e4898129"
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer)\n",
        "tfidf = tfidf_vectorizer.fit_transform(texts)\n",
        "tfidf_vectorizer.vocabulary_\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'аудитория': 0,\n",
              " 'будни': 1,\n",
              " 'бы': 2,\n",
              " 'быть': 3,\n",
              " 'великолепный': 4,\n",
              " 'вернуть': 5,\n",
              " 'всё': 6,\n",
              " 'если': 7,\n",
              " 'зрительский': 8,\n",
              " 'который': 9,\n",
              " 'круг': 10,\n",
              " 'любой': 11,\n",
              " 'лёгкий': 12,\n",
              " 'маленький': 13,\n",
              " 'на': 14,\n",
              " 'написать': 15,\n",
              " 'не': 16,\n",
              " 'нерв': 17,\n",
              " 'ниже': 18,\n",
              " 'общий': 19,\n",
              " 'он': 20,\n",
              " 'пара': 21,\n",
              " 'первый': 22,\n",
              " 'пожалуй': 23,\n",
              " 'положительный': 24,\n",
              " 'помочь': 25,\n",
              " 'посмотреть': 26,\n",
              " 'при': 27,\n",
              " 'просто': 28,\n",
              " 'рейтинг': 29,\n",
              " 'рецензия': 30,\n",
              " 'рука': 31,\n",
              " 'свой': 32,\n",
              " 'сезон': 33,\n",
              " 'сериал': 34,\n",
              " 'серый': 35,\n",
              " 'скрасить': 36,\n",
              " 'следующий': 37,\n",
              " 'создатель': 38,\n",
              " 'становиться': 39,\n",
              " 'стресс': 40,\n",
              " 'то': 41,\n",
              " 'только': 42,\n",
              " 'успокоить': 43,\n",
              " 'это': 44}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tr4AHMG_Ci8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7309a0a-bd7e-4ada-eb37-3a570e6a802e"
      },
      "source": [
        "TfidfVectorizer(sublinear_tf=True).fit_transform(texts).todense()[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[0.19275789, 0.19275789, 0.        , 0.19275789, 0.        ,\n",
              "         0.        , 0.19275789, 0.32636748, 0.19275789, 0.        ,\n",
              "         0.14659734, 0.19275789, 0.        , 0.19275789, 0.        ,\n",
              "         0.        , 0.32636748, 0.19275789, 0.        , 0.19275789,\n",
              "         0.        , 0.32636748, 0.19275789, 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.19275789, 0.        , 0.        , 0.19275789,\n",
              "         0.14659734, 0.        , 0.14659734, 0.        , 0.        ,\n",
              "         0.19275789, 0.19275789, 0.19275789, 0.        , 0.19275789,\n",
              "         0.        , 0.        , 0.14659734]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVE38ye4hqzc"
      },
      "source": [
        "https://ai.stanford.edu/~amaas/data/sentiment/\n"
      ]
    }
  ]
}